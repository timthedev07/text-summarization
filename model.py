# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xc-d9w7SLmYUPSxBRp03D33Qn_XaFPPU

Download the target dataset.
"""

import matplotlib.pyplot as plt
from google.colab import drive
from tensorflow.python.keras import backend as K
import os
import re
import numpy as np
import pandas as pd
import string
# for preprocessing
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from nltk.corpus import stopwords
import nltk
from bs4 import BeautifulSoup
# machine learning
from tensorflow.keras.layers import InputLayer, Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, GlobalAveragePooling1D
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split

"""Prepare the custom Attention Layer."""


logger = tf.get_logger()


class AttentionLayer(tf.keras.layers.Layer):
    """
    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).
    There are three sets of weights introduced W_a, U_a, and V_a
     """

    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        assert isinstance(input_shape, list)
        # Create a trainable weight variable for this layer.

        self.W_a = self.add_weight(name='W_a',
                                   shape=tf.TensorShape(
                                       (input_shape[0][2], input_shape[0][2])),
                                   initializer='uniform',
                                   trainable=True)
        self.U_a = self.add_weight(name='U_a',
                                   shape=tf.TensorShape(
                                       (input_shape[1][2], input_shape[0][2])),
                                   initializer='uniform',
                                   trainable=True)
        self.V_a = self.add_weight(name='V_a',
                                   shape=tf.TensorShape(
                                       (input_shape[0][2], 1)),
                                   initializer='uniform',
                                   trainable=True)

        # Be sure to call this at the end
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        """
        inputs: [encoder_output_sequence, decoder_output_sequence]
        """
        assert type(inputs) == list
        encoder_out_seq, decoder_out_seq = inputs

        logger.debug(f"encoder_out_seq.shape = {encoder_out_seq.shape}")
        logger.debug(f"decoder_out_seq.shape = {decoder_out_seq.shape}")

        def energy_step(inputs, states):
            """ Step function for computing energy for a single decoder state
            inputs: (batchsize * 1 * de_in_dim)
            states: (batchsize * 1 * de_latent_dim)
            """

            logger.debug("Running energy computation step")

            if not isinstance(states, (list, tuple)):
                raise TypeError(
                    f"States must be an iterable. Got {states} of type {type(states)}")

            encoder_full_seq = states[-1]

            """ Computing S.Wa where S=[s0, s1, ..., si]"""
            # <= batch size * en_seq_len * latent_dim
            W_a_dot_s = K.dot(encoder_full_seq, self.W_a)

            """ Computing hj.Ua """
            U_a_dot_h = K.expand_dims(
                K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim

            logger.debug(f"U_a_dot_h.shape = {U_a_dot_h.shape}")

            """ tanh(S.Wa + hj.Ua) """
            # <= batch_size*en_seq_len, latent_dim
            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)

            logger.debug(f"Ws_plus_Uh.shape = {Ws_plus_Uh.shape}")

            """ softmax(va.tanh(S.Wa + hj.Ua)) """
            # <= batch_size, en_seq_len
            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)
            # <= batch_size, en_seq_len
            e_i = K.softmax(e_i)

            logger.debug(f"ei.shape = {e_i.shape}")

            return e_i, [e_i]

        def context_step(inputs, states):
            """ Step function for computing ci using ei """

            logger.debug("Running attention vector computation step")

            if not isinstance(states, (list, tuple)):
                raise TypeError(
                    f"States must be an iterable. Got {states} of type {type(states)}")

            encoder_full_seq = states[-1]

            # <= batch_size, hidden_size
            c_i = K.sum(encoder_full_seq * K.expand_dims(inputs, -1), axis=1)

            logger.debug(f"ci.shape = {c_i.shape}")

            return c_i, [c_i]

        # we don't maintain states between steps when computing attention
        # attention is stateless, so we're passing a fake state for RNN step function
        fake_state_c = K.sum(encoder_out_seq, axis=1)
        # <= (batch_size, enc_seq_len, latent_dim
        fake_state_e = K.sum(encoder_out_seq, axis=2)

        """ Computing energy outputs """
        # e_outputs => (batch_size, de_seq_len, en_seq_len)
        last_out, e_outputs, _ = K.rnn(
            energy_step, decoder_out_seq, [
                fake_state_e], constants=[encoder_out_seq]
        )

        """ Computing context vectors """
        last_out, c_outputs, _ = K.rnn(
            context_step, e_outputs, [
                fake_state_c], constants=[encoder_out_seq]
        )

        return c_outputs, e_outputs

    def compute_output_shape(self, input_shape):
        """ Outputs produced by the layer """
        return [
            tf.TensorShape(
                (input_shape[1][0], input_shape[1][1], input_shape[1][2])),
            tf.TensorShape(
                (input_shape[1][0], input_shape[1][1], input_shape[0][1]))
        ]


"""Ensure that the corpus `stopwords` is downloaded."""

nltk.download('stopwords')

"""Some constants."""

DATA_DIR = "data"
CSV_FILENAME = "reviews.csv"
SUMMARY_TOKEN_START = "_START_"
SUMMARY_TOKEN_END = "_END_"

"""Read the dataset."""

pathJoin = os.path.join


def readDataset():
    df = pd.read_csv(pathJoin(DATA_DIR, CSV_FILENAME),
                     usecols=["Summary", "Text"])
    df.drop_duplicates(subset=['Text'], inplace=True)
    df.dropna(axis=0, inplace=True)
    return df


"""Function for embedding special tokens into our $y$ values(summaries) to indicate the start and end of a sequence."""


def embedToken(summary: str):
    return f"{SUMMARY_TOKEN_START} {summary} {SUMMARY_TOKEN_END}"


"""Data cleaning."""

stop_words = set(stopwords.words('english'))
if "not" in stop_words:
    stop_words.remove("not")


def decontracted(phrase: str):
    """
    https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python
    """
    phrase = re.sub(r"won\'t", "will not", phrase)
    phrase = re.sub(r"can\'t", "can not", phrase)

    phrase = re.sub(r"n\'t", " not", phrase)
    phrase = re.sub(r"\'re", " are", phrase)
    phrase = re.sub(r"\'s", " is", phrase)
    phrase = re.sub(r"\'d", " would", phrase)
    phrase = re.sub(r"\'ll", " will", phrase)
    phrase = re.sub(r"\'t", " not", phrase)
    phrase = re.sub(r"\'ve", " have", phrase)
    phrase = re.sub(r"\'m", " am", phrase)
    return phrase


def textClean(text: str):
    text = decontracted(text.lower())
    text = BeautifulSoup(text, "html.parser").text

    text = re.sub(r"\d+(?:\.\d*)?(?:[eE][+-]?\d+)?", " ", text)
    text = re.sub(r"@([A-Za-z0-9_]+)", " ", text)
    text = re.sub(r"\([^)]*\)", "", text)

    for i in stop_words:
        text = re.sub(f'[^A-Za-z0-9_]+{i}[^A-Za-z0-9_]+', " ", text)

    text = re.sub(
        f"[{re.escape(string.punctuation)}]", "", text
    )

    return text


def getCleanedDataset(dataset: pd.DataFrame, savedCSVPath=None) -> pd.DataFrame:
    """
    Takes a dataframe containing these two required two columns:
        - Summary
        - Text

    The param `savedCSVPath` is used to indicate the path to the saved CSV file containing the cleaned data if this step is previously done.

    The output data frame contains the cleaned texts stored in the following rows:
        - summaryCleaned
        - textCleaned
    """
    if savedCSVPath:
        return pd.read_csv(savedCSVPath)

    summaryCleaned = [embedToken(textClean(i)) for i in dataset["Summary"]]
    textCleaned = [textClean(i) for i in dataset["Text"]]

    cleaned = pd.DataFrame()
    cleaned["summaryCleaned"] = summaryCleaned
    cleaned["textCleaned"] = textCleaned

    noAlphabet = r"^[^a-zA-Z]+$"

    cleaned['summaryCleaned'].replace(
        noAlphabet, np.nan, inplace=True, regex=True)
    cleaned['textCleaned'].replace(
        noAlphabet, np.nan, inplace=True, regex=True)
    cleaned.dropna(axis=0, inplace=True)

    return cleaned


getCleanedDataset(readDataset().head())

"""Load all the data"""

# Commented out IPython magic to ensure Python compatibility.
# allData = getCleanedDataset(readDataset())
drive.mount('/content/gdrive')

!cp "/content/gdrive/My Drive/datasets/text-summarization/cleaned.csv.zip" data
# %cd data
!unzip - ojq cleaned.csv.zip
!rm - rf cleaned.csv.zip
# %cd ..
allData = getCleanedDataset(
    None, savedCSVPath=pathJoin(DATA_DIR, "cleaned.csv"))

"""Investigate the lengths of our input data."""

text_word_count = []
summary_word_count = []

# populate the lists with sentence lengths
for i in allData['textCleaned']:
    text_word_count.append(len(i.split()))

for i in allData['summaryCleaned']:
    summary_word_count.append(len(i.split()))

length_df = pd.DataFrame({
    'text': text_word_count,
    'summary': summary_word_count
})
length_df.hist(bins=30)
plt.show()

"""Export and compress the cleaned data to my drive.

**_Only run this step if there isn't a pre-processed csv file._**
"""

allData.to_csv(pathJoin(
    DATA_DIR, "/content/gdrive/My Drive/datasets/text-summarization/cleaned.csv"), index=False)
!zip - r "/content/gdrive/My Drive/datasets/text-summarization/cleaned.csv.zip" "/content/gdrive/My Drive/datasets/text-summarization/cleaned.csv"
!rm - rf "/content/gdrive/My Drive/datasets/text-summarization/cleaned.csv"

"""NOTE: the pre-processed csv can also be found [here](https://drive.google.com/file/d/1-01QGJvxBE-i9MsA5-PUVXSW1VbF4zBP/view?usp=sharing).

Prepare training and testing data.
"""

amount = 100000
trainX, testX, trainY, testY = train_test_split(
    allData["textCleaned"][:amount], allData["summaryCleaned"][:amount], test_size=0.2, random_state=42)

"""Model building."""


class TextSummarization:
    def __init__(self, _trainX, _trainY, _testX, _testY):
        """
        The x and y values passed in to this function should simply be processed texts
        in the form of arrays of plain strings. More processing will be done on the data
        underneath the hood of this class.
        """
        self.trainX = _trainX
        self.trainY = _trainY
        self.testX = _testX
        self.testY = _testY

        self.textTokenizer = Tokenizer(num_words=9000)
        self.summaryTokenizer = Tokenizer(num_words=4000)

        self.TEXT_MAX_LEN = 30
        self.SUMMARY_MAX_LEN = 9

        self.textVocabSize = None
        self.summaryVocabSize = None

        self.latentDim = 300

        self.modelToTrain = None

    def tokenize(self):
        # training the tokenizers
        self.textTokenizer.fit_on_texts(list(self.trainX))
        self.summaryTokenizer.fit_on_texts(list(self.trainY))

        # tokenize the data
        self.trainX = self.textTokenizer.texts_to_sequences(self.trainX)
        self.testX = self.textTokenizer.texts_to_sequences(self.testX)

        self.trainY = self.summaryTokenizer.texts_to_sequences(self.trainY)
        self.testY = self.summaryTokenizer.texts_to_sequences(self.testY)

        # apply padding to unify the shape
        self.trainX = pad_sequences(
            self.trainX, maxlen=self.TEXT_MAX_LEN, padding="post")
        self.testX = pad_sequences(
            self.testX, maxlen=self.TEXT_MAX_LEN, padding="post")

        self.trainY = pad_sequences(
            self.trainY, maxlen=self.SUMMARY_MAX_LEN, padding="post")
        self.testY = pad_sequences(
            self.testY, maxlen=self.SUMMARY_MAX_LEN, padding="post")

        self.textVocabSize = self.textTokenizer.num_words + 1
        self.summaryVocabSize = self.summaryTokenizer.num_words + 1

    def initEncoderDecoder(self):
        """
        Initializes the encoder-decoder architecture and populates self.modelToTrain
        which represents the full architecture.
        """

        # initialize encoder
        encoderInput = Input(shape=(self.TEXT_MAX_LEN, ), dtype='int32')
        encoderEmbedded = Embedding(self.textVocabSize, self.latentDim, trainable=True, input_shape=(
            self.TEXT_MAX_LEN, ))(encoderInput)

        def genLSTM(): return LSTM(self.latentDim, return_sequences=True, return_state=True)

        encLSTMOut1, stateH1, stateC1, = genLSTM()(encoderEmbedded)
        encLSTMOut2, stateH2, stateC2, = genLSTM()(encLSTMOut1)
        encoderOutput, encoderFinalStateH, encoderFinalStateC, = genLSTM()(encLSTMOut2)

        # initialize decoder
        decoderInput = Input(shape=(None, ), dtype='int32')
        decoderEmbedded = Embedding(
            self.summaryVocabSize, self.latentDim, trainable=True)(decoderInput)
        decoderLSTMOutput, _, __ = genLSTM()(decoderEmbedded, initial_state=[
            encoderFinalStateH, encoderFinalStateC])

        attentionOutput, _ = AttentionLayer()(
            [encoderOutput, decoderLSTMOutput])

        concatenated = Concatenate(
            axis=-1)([decoderLSTMOutput, attentionOutput])

        outputLayer = TimeDistributed(
            Dense(self.summaryVocabSize, activation="softmax"))
        decoderOutput = outputLayer(concatenated)

        model = Model([encoderInput, decoderInput], decoderOutput)
        model.compile(
            optimizer="rmsprop",
            loss="sparse_categorical_crossentropy",
        )
        self.modelToTrain = model

    def trainModel(self):
        callback = EarlyStopping(monitor="val_loss", mode="min")
        self.modelToTrain.fit(
            [self.trainX, self.trainY[:, :-1]],
            self.trainY.reshape(
                self.trainY.shape[0], self.trainY.shape[1], 1)[:, 1:],
            epochs=20,
            callbacks=[callback],
            batch_size=512,
            validation_data=(
                [self.testX, self.testY[:, :-1]],
                self.trainY.reshape(
                    self.trainY.shape[0], self.trainY.shape[1], 1)[:, 1:]
            )
        )


t = TextSummarization(trainX, trainY, testX, testY)
t.tokenize()
t.initEncoderDecoder()

# t.modelToTrain.summary()
t.trainModel()
# t.summaryVocabSize
